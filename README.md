# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."** <br>
The dataset contains data about bank customers. And the concern is whrther the customer avails a particular service ( which servicee I am not sure) or not. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**<br>
The best model in this case is a voting ensamble by AutoML with 1% more accuracy than Hyperdrive outcome.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**<br>
The environment I have used for this experiment is already available in Azure.<br>
So sklearn_env = Environment.get(workspace=ws, name='AzureML-sklearn-0.24-ubuntu18.04-py37-cuda11-gpu') line gets that environment for my ML workspace.<br>
The algorithm used is **Logistic Regression** There are 2 hyperparameters to select one is C ( which has default value of 1) and other is max-iter (which has default value of 100). <br>
As we know default values are good enough so we don't need to tweak it a lot The best value is chosen using hyperdrive.

**What are the benefits of the parameter sampler you chose?**<br>
I have used Random parameter sampler. The benefit is it is simple and runs fast gives almost same accuracy as Grid sampler. I have not used baysian sampler because it is not required for selecting 2 hyperparameters on a simple model like logistic regression and early termination does not work with Baysian sampler.<br>

**What are the benefits of the early stopping policy you chose?**<br>
I am using Bandit policy, benefit is it is highly flixible as I can choose below what metric I want to terminate the run (unlike median policy which terminates only based on median value).<br>

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**<br>
1. *experiment_timeout_minutes* is self explanatory. It describies the number of minutes the experiment should run. default is 6 days. So within 30 minutes whatever best model we get that will be out output model.
2. *task* there are many tasks that can be configured for an automl run from forcasting, regression to different DNN tasks like  'image-object-detection', 'text-ner' etc for our task we are using "classification".
3. *primary_metric* There are many metrics by which the model performance can be measured. Now all metric is not applicable for all tasks, for example for a regression task "Accuracy" is not the right choice. Azure has created defaault primary metric for each task. For example for classification it is "Accuracy", for regression it is "normalized root mean squared" etc.
4. *training_data* another self explanatory parameter. The dataset should contain both the features and the labels.
5. *label_column_name* a must use parameter with training data. As the training data contains both features and labels we need to specify which column represents the label of that instance.
6. *compute_target* Where the autoML run is going to take place it can be both local and remote. Although in our case the compute target is remote which we have created earlier.
7. *max_cores_per_iteration* Maxinumber of cores to be used for a training iteration. Default value is 1, if we set it as -1 azure will utilize as many as possible. This value should not be more than the number of cores available for at compute_target. As my compute_target has 4 cores I have assigned it as 4.
8. *max_concurrent_iterations* The number of parallel iterations that we want to run. In our case the instance we are using is an AMLCompute cluster, and this kind of cluster supports one ieration running per node, so this parameter is invaluable to utilise all the cores of the compute cluster. Ideally the value for this parameter should be between 1 and Number of cores available (thats why I have used 4). But we can use more than the number of cores as well in that case the jobs will be queued.
9. *n_cross_validations* number of cross validations to use if validation data is not mentioned. In our case as we have not mentioned validation data seperately I think 4 fold cross validation is reasonable enough.<br>
<br>
**Outcome of the Model** 
The automl best model is a voting ensemble model which consists of 8 different boosting models. Each has different amount of say the most with 0.21 and least with 0.07. The boosting algorithms are either XGBoost ot LightGBM. Prior to applying those algorithms different normalizers are used. So it is clear that autoML takes care of feature engineering before feeding it to a model.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?** <br>
The AutoML mode gave slightly more accuracy (91.7%) than hyperdrive (91.3%). The biggest difference is AutoML do not need any hyperparameter tuning or training script. Those are automated hence AutoML.<br>

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?** <br>
The biggest improved I can think of is feature engineering in scikit learn model. Currently we are feeding the data directly to a model. This data must have some patterns and reationship among the features. We can find few of those and decrease the number of features. Even if we can merge the similar jobs into one category that will significanly decrease number of features. As we are submitting the data with numeric and categorical columns it is certain that azure is taking care of its categorical data encoding and scaling. AutoML does feature engineering but whether HyperDrive does the same of is it limited to basic feature engineering I am not sure about it.<br>
So if the review can clarify whether HyperDrive does feature engineering as good as autoML or it does bare minimum to run the required model, it will be very helpful.<br>
But to get better predictions from scikit-learn I would go for feature engineering.
