# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."** <br>
The dataset contains data about bank customers. And the concern is whrther the customer avails a particular service ( which servicee I am not sure) or not. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**<br>
The best model in this case is a voting ensamble by AutoML with 1% more accuracy than Hyperdrive outcome.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**<br>
The environment I have used for this experiment is already available in Azure.<br>
So sklearn_env = Environment.get(workspace=ws, name='AzureML-sklearn-0.24-ubuntu18.04-py37-cuda11-gpu') line gets that environment for my ML workspace.<br>
The algorithm used is **Logistic Regression** There are 2 hyperparameters to select one is C ( which has default value of 1) and other is max-iter (which has default value of 100). <br>
As we know default values are good enough so we don't need to tweak it a lot The best value is chosen using hyperdrive.

**What are the benefits of the parameter sampler you chose?**<br>
I have used Random parameter sampler. The benefit is it is simple and runs fast gives almost same accuracy as Grid sampler. I have not used baysian sampler because it is not required for selecting 2 hyperparameters on a simple model like logistic regression and early termination does not work with Baysian sampler.<br>

**What are the benefits of the early stopping policy you chose?**<br>
I am using Bandit policy, benefit is it is highly flixible as I can choose below what metric I want to terminate the run (unlike median policy which terminates only based on median value).<br>

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The automl best model is a voting ensemble model which consists of 6 different xgboost models. Each has different amount of say the most with 0.25 and least with 0.08.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?** <br>
The AutoML mode gave slightly more accuracy (92%) than hyperdrive (91.3%). The biggest difference is AutoML do not need any hyperparameter tuning or training script. Those are automated hence AutoML.<br>

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?** <br>
The biggest improved I can think of is feature engineering in scikit learn model we are feeding the data directly to a model. I think with proper feature engineering the accuracy can go up significantly.

